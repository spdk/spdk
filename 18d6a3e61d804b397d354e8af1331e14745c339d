{
  "comments": [
    {
      "key": {
        "uuid": "74642a90_a5df706b",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 79,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "Use svm_queue_t instead of unix_shared_memory_queue_t. The latter is a typedef to the former and it will be eventually removed.",
      "range": {
        "startLine": 79,
        "startChar": 7,
        "endLine": 79,
        "endChar": 33
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9a197de6_5629eb6a",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 79,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T17:27:17Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "74642a90_a5df706b",
      "range": {
        "startLine": 79,
        "startChar": 7,
        "endLine": 79,
        "endChar": 33
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6a261ad3_35cb0d44",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 94,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "You may consider using foreach_app_session_field just as vcl_session_t does. The benefit is that you\u0027ll be able to use app_recv*/app_send* which work with both stream and dgram sessions.",
      "range": {
        "startLine": 94,
        "startChar": 7,
        "endLine": 94,
        "endChar": 23
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "bb5fb908_90f15094",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 94,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T18:38:04Z",
      "side": 1,
      "message": "Yes, this is good idea to unify it, also for readability.",
      "parentUuid": "6a261ad3_35cb0d44",
      "range": {
        "startLine": 94,
        "startChar": 7,
        "endLine": 94,
        "endChar": 23
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "bd7d46aa_9e48c1ae",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 165,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "Two comments here:\n1. Have you considered using per spdk worker session pools? That way you could avoid locking the pool for any CRD operation. \n2. Assuming 1. cannot be done: I never measured performance but I suspect pthread_mutex_lock may be a bit slower than clib_spinlock_lock. Do you expect levels of contention here whereby sleeping is better than spinning?",
      "range": {
        "startLine": 165,
        "startChar": 1,
        "endLine": 165,
        "endChar": 19
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e7837795_efa06f5b",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 165,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T18:38:04Z",
      "side": 1,
      "message": "We do not do this lookup very often, but per worker session pool is a good idea worth to consider. For now we\u0027re more focused on the fast path (recv/write) when connection is established.",
      "parentUuid": "bd7d46aa_9e48c1ae",
      "range": {
        "startLine": 165,
        "startChar": 1,
        "endLine": 165,
        "endChar": 19
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ec6a65cf_dec2754a",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 487,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "I think TRANSPORT_PROTO_TCP is visible here.",
      "range": {
        "startLine": 487,
        "startChar": 14,
        "endLine": 487,
        "endChar": 15
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "715b120c_0169b867",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 487,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T17:27:17Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "ec6a65cf_dec2754a",
      "range": {
        "startLine": 487,
        "startChar": 14,
        "endLine": 487,
        "endChar": 15
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b4048a43_66079a1b",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 917,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "This is fine. But, to clarify things, since I don\u0027t exactly know how SPDK works: is the buf really the destination or an intermediary buffer? If it\u0027s an intermediary buffer, instead of doing the copy, you could retrieve pointers to the data in the fifo with svm_fifo_segments (see vppcom_session_read_segments for an example, unfortunately only recently introduced). You can pass the segments around until you finally do the copy. The only downside is that once you do the copy, you need to call svm_fifo_segments_free, to mark the fifo as free.",
      "range": {
        "startLine": 917,
        "startChar": 6,
        "endLine": 917,
        "endChar": 29
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "31e1043d_c7ec6dc0",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 917,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T18:38:04Z",
      "side": 1,
      "message": "Yes, this is good idea to limit memcpy\u0027s, but will require the changes in our API and iSCSI application. Now it is used to store e.g. incoming iSCSI PDU and iSCSI implementation expects that this memory is not fragmented. But this is definitely worth to consider for future improvements.",
      "parentUuid": "b4048a43_66079a1b",
      "range": {
        "startLine": 917,
        "startChar": 6,
        "endLine": 917,
        "endChar": 29
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8b3ab445_0e33fdd9",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 958,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:21:48Z",
      "side": 1,
      "message": "Maybe it would be clearer if you used SVM_Q_WAIT here.",
      "range": {
        "startLine": 958,
        "startChar": 54,
        "endLine": 958,
        "endChar": 83
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "f7e577d4_c3823714",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 958,
      "author": {
        "id": 1013008
      },
      "writtenOn": "2018-09-25T17:27:17Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "8b3ab445_0e33fdd9",
      "range": {
        "startLine": 958,
        "startChar": 54,
        "endLine": 958,
        "endChar": 83
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c5d4e772_3a7149f8",
        "filename": "lib/sock/vpp/vpp.c",
        "patchSetId": 31
      },
      "lineNbr": 1065,
      "author": {
        "id": 1015094
      },
      "writtenOn": "2018-09-23T19:57:24Z",
      "side": 1,
      "message": "I saw this only after sending the previous set of comments. This is not exactly fast but as long as you don\u0027t have many sessions you may not see a difference. \n\nThe thing to note is that fifos have a \"has_event\" field. If not set, on rx of new data, vpp generates a FIFO_EVENT_APP_RX event message and sends it on the app\u0027s event queue (g_app_event_queue). Therefore, instead of scanning all the socks, you could just dequeue all the io event messages, generate events from them and unset has_event. You\u0027ll probably need to handle situations when you receive an event and the fifo is actually empty. The scenario goes like this: you generate an event from an io event message, unset has_event, you get a new io event message before recv consumes the data, recv consumes all the data (not only what was signaled by the first event), if you\u0027re not careful you may generate a new event from the second io event message, although the fifo is empty because no new data has been received (latest version of vcl deals with that, but it also uses the new message queues). \n\nEven if you don\u0027t use the io event messages, you should still make sure you clean up g_app_event_queue. But in this case, don\u0027t unset has_event.\n\nFinally, if you want to block waiting for events here, you can call svm_queue_wait on the app\u0027s event queue. That eventually leads to a pthread_mutex_cond_wait on the queue\u0027s mutex which pops when the queue goes non-empty.",
      "range": {
        "startLine": 1065,
        "startChar": 0,
        "endLine": 1065,
        "endChar": 29
      },
      "revId": "18d6a3e61d804b397d354e8af1331e14745c339d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}