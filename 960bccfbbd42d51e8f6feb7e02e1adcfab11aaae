{
  "comments": [
    {
      "key": {
        "uuid": "c1f322a9_b4b9d8a4",
        "filename": "examples/nvme/perf/perf.c",
        "patchSetId": 2
      },
      "lineNbr": 516,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2017-08-02T22:26:48Z",
      "side": 1,
      "message": "Hi Gang,\n\nWhy do we need to do this check here?\n\nI think it is OK if we count one or two extra I/O.  If we are testing for 60 seconds, we probably have completed millions of I/O, so one or two extra do not make any real difference.\n\nI think this just makes the code more complicated.  It also adds more calls to spdk_get_ticks() which are not cheap.  So it could actually hurt performance in the case where we are trying to get 3-4 million I/O on one core.",
      "revId": "960bccfbbd42d51e8f6feb7e02e1adcfab11aaae",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "7ac3398d_802e954b",
        "filename": "examples/nvme/perf/perf.c",
        "patchSetId": 2
      },
      "lineNbr": 516,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2017-08-02T22:34:06Z",
      "side": 1,
      "message": "Ignore my comment on additional calls to spdk_get_ticks().  Daniel pointed out to me offline that you add one on line 511 but removed the one from line 514 in the old file.\n\nI am still not sure the extra complication is worth it though.  Do you see a use case where it is critical that we get the exact number of I/O?\n\nWorst case, if you run QD\u003d128, you could get 128 extra I/O which complete just after TSC expires.  If the device only does 50K IO/s, and runs for 10 seconds, the performance impact would be 128 / (50K * 10s) \u003d 0.02%.  For longer test runs, devices with higher throughput, or lower queue depth, the % difference would be even smaller.",
      "parentUuid": "c1f322a9_b4b9d8a4",
      "revId": "960bccfbbd42d51e8f6feb7e02e1adcfab11aaae",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c4dcbab6_cef92775",
        "filename": "examples/nvme/perf/perf.c",
        "patchSetId": 2
      },
      "lineNbr": 516,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2017-08-03T01:41:39Z",
      "side": 1,
      "message": "The issue happens under the multi devices case for example, we run perf with 16 NVMe SSDs for a short time. The original behavior will count in more IOs as we loop all the devices (namespaces) and then check the time to break out. In the fixed way, add a local variable to check and properly count in those valid IOs and set the is_draining flag if time is expired.\n\nWenzhong verified the result and show no different on 16 NVMe SSDs with short run and long run:\n\nhttps://jira01.devtools.intel.com/browse/CSS-779",
      "parentUuid": "7ac3398d_802e954b",
      "revId": "960bccfbbd42d51e8f6feb7e02e1adcfab11aaae",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    }
  ]
}