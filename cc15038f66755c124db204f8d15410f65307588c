{
  "comments": [
    {
      "key": {
        "uuid": "ec010541_1c8d1ce6",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T18:02:40Z",
      "side": 1,
      "message": "Does posting a receive after placing the qp in the error state work? I was testing and only posting a send actually worked - the recv would just fail to post here. I need to go back and test on both rxe and on mlx stacks though. It could be another rxe bug.",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e3a85f0c_e8e0da9d",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T18:15:11Z",
      "side": 1,
      "message": "I just confirmed - calling ibv_post_recv fails if the qp is in the error state. This is true for both the mlx and rxe stacks. In my patch series I use the dummy command to determine when send completions are done, but on the receive side I wait for the cq to drain entirely one time after placing the qp in the error state and then consider the receives done. This seems to be working.",
      "parentUuid": "ec010541_1c8d1ce6",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c19cfa43_e149bd49",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T20:45:22Z",
      "side": 1,
      "message": "You can see from the test logs that your patch did indeed hit the assert on line 2677 because posting the recv failed.",
      "parentUuid": "e3a85f0c_e8e0da9d",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "7accb822_f68bb299",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-28T20:59:35Z",
      "side": 1,
      "message": "Posting receive after placing the QP in the error state should work. We\u0027ve tested that in both mlnx and soft-roce stack. \n\nIn which log I can see backtrace from the assert?",
      "parentUuid": "c19cfa43_e149bd49",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "93b0a22e_039731c1",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-29T15:33:24Z",
      "side": 1,
      "message": "I was wrong here - crossed two different logs. The receive failed to post on the other log because that particular patch didn\u0027t add an additional recv queue entry as you indicated. Your patch successfully posted the receive, but failed when it went to destroy the qpair because the queue depth was not zero.\n\nhttps://ci.spdk.io/spdk/builds/review/cc15038f66755c124db204f8d15410f65307588c.1540494258/fedora-03/build.log\n\nSo you are right - posting the receive does work. Let\u0027s go with that strategy then. In terms of my other patch series, I\u0027ll just drop the last patch from my series (because I like the other patches) and we\u0027ll put this patch on top.",
      "parentUuid": "7accb822_f68bb299",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "087cffde_daee41f0",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2644,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-29T17:06:53Z",
      "side": 1,
      "message": "OK. Thanks.\nI\u0027ll see the log tomorrow. \n\nOnce, you merge your series, we will rebase this patch.\nBut we still have an issue with rxe. \nI don\u0027t quite understand how we can deal with it without removing assert for qp_num",
      "parentUuid": "93b0a22e_039731c1",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3ff24eae_92f911bc",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2741,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T18:02:40Z",
      "side": 1,
      "message": "What if the type is a regular request or recv? Can you only get flush errors if the connection is shutting down? Are we guaranteed to get an RDMA disconnect event any time we see flush errors?",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "8c15366a_f39f5100",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2741,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-28T20:59:35Z",
      "side": 1,
      "message": "QP may transitioned to error state automatically by HW. The first WR that complied with error will indicate a reason for error. Any new WR$ posted to QP in error state will be finished with flash error. I\u0027m not sure about WR posted before error, but not which one completed with error. I think, they will also finished with flush error.\n\nAre you asking about RDMA_CM_EVENT_DISCONNECTED in librdmacm? \nI should check that. I believe,we will not receive RDMA_CM_EVENT_DISCONNECTED in all cases. The local port can go down and remote side don\u0027t know about that till sending data. librdamcm exchange information using some kind of \"out-of-band\". In case of Infiniband, I relies on mcast group created for IpovIB. So, I think, if we close librdmacm\u0027s channel by rdma_disconnect, both sides receive RDMA_CM_EVENT_DISCONNECTED. But that\u0027s not related to transitioning to error state in RDMA QP.",
      "parentUuid": "3ff24eae_92f911bc",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0a6cb367_55c4de96",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2745,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-29T08:03:50Z",
      "side": 1,
      "message": "This assert should be removed. qp_num can be invalid in case of error",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "322dff02_e1305aae",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2745,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-29T15:33:24Z",
      "side": 1,
      "message": "The man page for ibv_poll_cq says wr_id, status, qp_num, and vendor_err are valid on non-success, and from my experience (outside of rxe where nothing was valid and we have a kernel patch to fix it) this is true.",
      "parentUuid": "0a6cb367_55c4de96",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e434ac58_b4d98044",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2745,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-29T17:02:46Z",
      "side": 1,
      "message": "OK. I see the the phrase in \"man ibv_poll_cq\" you refer:\n\" Not all wc attributes are always valid. If the completion status is other than IBV_WC_SUCCESS, only the following attributes are valid: wr_id, status, qp_num, and vendor_err.\"\n\nI can\u0027t find in Infiniband spec the basis for the statement (in case of qp_num) . But, I agree that non-rxe stack returns valid qp num.\n\nEven if the kernel fix is accepted, on old kernels and old OFEDs the assert will fail on rxe. Is that OK?\nFrom our perspective (Mellanox), we should focus on real case: Infiniband, ROcE, iWrap. But that can be problem for CI based on rxe.",
      "parentUuid": "322dff02_e1305aae",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "dcffcff4_81e30649",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2788,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T18:02:40Z",
      "side": 1,
      "message": "There is a bug in the rxe driver where it doesn\u0027t fill out the wc here at all on error. I notified the linux-rdma mailing list and Sagi Grimberg posted a fix for it. When I apply that fix locally, everything works as expected. This was a major source of confusion as I implemented this, so I\u0027m glad it is cleared up.\n\nUnfortunately, the test machines don\u0027t have the patch applied, meaning we can\u0027t do a correct shutdown on the rxe machines. I\u0027m not sure how to deal with this just yet.",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "fa74ba77_292995ae",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2788,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-28T21:10:48Z",
      "side": 1,
      "message": "Can you send me link to the fix. I\u0027ll care to push it to soft-roce in Mellanox OFED. \n\nI\u0027ll see the fix, but I think, we can differentiate rxe and mlnx in configure time and disable code that breaks tests. I\u0027ve never heard about soft-roce in production. But today closing procedure is wrong, nvmf-target can crash in real scenario.",
      "parentUuid": "dcffcff4_81e30649",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6bcc1a6b_aca02b47",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2788,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-29T08:03:50Z",
      "side": 1,
      "message": "Hi,\n\nI found the fix in linux-rdma list. I think, existing code in rxe can be improved, but it\u0027s correct.\n - Infiniband spec dosen\u0027t require qp_num to be valid in case of error. Only wr_id and status must be valid\n - wr_id is set to valid value\n\nstruct ib_wc {                                                                                \n        union {                                                                               \n                u64             wr_id;                                                        \n                struct ib_cqe   *wr_cqe;                                                      \n        };                                                                                    \n        enum ib_wc_status       status;                                                       \n        enum ib_wc_opcode       opcode;                                                       \n        u32                     vendor_err;                                                   \n        u32                     byte_len;                                                     \n        struct ib_qp           *qp;                                                           \n        union {                                                                               \n                __be32          imm_data;                                                     \n                u32             invalidate_rkey;                                              \n        } ex;                                                                                 \n        u32                     src_qp;                                                       \n        int                     wc_flags;                                                     \n        u16                     pkey_index;                                                   \n        u32                     slid;                                                         \n        u8                      sl;                                                           \n        u8                      dlid_path_bits;                                               \n        u8                      port_num;       /* valid only for DR SMPs on switches */      \n        u8                      smac[ETH_ALEN];                                               \n        u16                     vlan_id;                                                      \n        u8                      network_hdr_type;                                             \n};\n\nstruct ib_uverbs_wc {\n        __u64 wr_id;\n        __u32 status;\n        __u32 opcode;\n        __u32 vendor_err;\n        __u32 byte_len;\n        union {\n                __u32 imm_data;\n                __u32 invalidate_rkey;\n        } ex;\n        __u32 qp_num;\n        __u32 src_qp;\n        __u32 wc_flags;\n        __u16 pkey_index;\n        __u16 slid;\n        __u8 sl;\n        __u8 dlid_path_bits;\n        __u8 port_num;\n        __u8 reserved;\n};\n\nstruct rxe_cqe {\n        union {\n                struct ib_wc            ibwc;\n                struct ib_uverbs_wc     uibwc;\n        };\n};\n\nExisting statement\nwc-\u003ewr_id\t\t\u003d wqe-\u003ewr_id;\nsets the valid wr_id for both cases: mlnx and rxe\n\nI think, \nassert(rqpair-\u003ecm_id-\u003eqp-\u003eqp_num \u003d\u003d wc-\u003eqp_num); line 2745\nin our original patch should be removed.",
      "parentUuid": "fa74ba77_292995ae",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c1e7911b_767ac361",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2798,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-10-25T18:02:40Z",
      "side": 1,
      "message": "I realized only yesterday that the opcode is not valid on error, so the whole block of code from here down isn\u0027t right. That was the source of what I thought were invalid wr_id\u0027s - I was using the opcode to determine the type and that\u0027s not allowed. I like the way you\u0027ve made wr_id point at an intermediate structure that contains the type to solve the problem.\n\nI\u0027m not currently sure whether the code should ever be initiating the disconnect process in response to finding an error here. Should we drive all disconnects via the rdma_cm_event channel? Is there ever an error we detect here that doesn\u0027t result in an event on the event channel?",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1dd8ddab_660c6bf5",
        "filename": "lib/nvmf/rdma.c",
        "patchSetId": 1
      },
      "lineNbr": 2798,
      "author": {
        "id": 1014643
      },
      "writtenOn": "2018-10-28T21:10:48Z",
      "side": 1,
      "message": "Does NVME-OF standard says something about QP disconnect? \nI believe, we should call explicitly to rdma_diconnect. This is a way to notify connected host about closing QP \"out-of-band\".\nLet me do some homework. I\u0027ll return with an answer.",
      "parentUuid": "c1e7911b_767ac361",
      "revId": "cc15038f66755c124db204f8d15410f65307588c",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    }
  ]
}