{
  "comments": [
    {
      "key": {
        "uuid": "c8893a04_a233258d",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 165,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-08-29T16:51:39Z",
      "side": 1,
      "message": "Why do you check for qid \u003e 0 here? Why exclude the admin queue pair? This needs a comment.",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "029e34c8_5f7f56db",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 165,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2018-08-30T00:29:47Z",
      "side": 1,
      "message": "Sure. Here its for the I/O operations so that needs to check the status of the subsystem. Admin requests like Set/Get Props can still be executed.",
      "parentUuid": "c8893a04_a233258d",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "f969ec17_923ffb93",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 165,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-08-30T17:01:03Z",
      "side": 1,
      "message": "Is there ever a cause where the subsystem is inactive and a valid command comes in? I don\u0027t believe there is. If the subsystem is inactive that means we just destroyed it.",
      "parentUuid": "029e34c8_5f7f56db",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d961d765_ac8575c7",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 165,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2018-08-31T00:09:01Z",
      "side": 1,
      "message": "The case is for the NULL IO QPair where 32 cores are assigned. The subsystem will keep in the inactive state due to NULL IO QPair found. It\u0027s ok to handle the admin queue here also.",
      "parentUuid": "f969ec17_923ffb93",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "dd877eb0_82ceeefd",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 173,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-08-29T16:51:39Z",
      "side": 1,
      "message": "There is a generic status type and an internal error code I believe. This doesn\u0027t seem like a privilege issue, which is what you are indicating with this error code.",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0a95eb2e_e8e6f8c0",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 173,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2018-08-30T00:29:47Z",
      "side": 1,
      "message": "I\u0027ve tried the Generic status and Internal error (and other possible errors). At the initiator side, when receiving this error, it could just queue the I/O and prevent other operations. So that return this error as the Subsystem is not ready and could not handle the required I/O operation.",
      "parentUuid": "dd877eb0_82ceeefd",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b042a560_fdb7d35b",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 173,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-08-30T17:01:03Z",
      "side": 1,
      "message": "I don\u0027t think this strategy is going to be robust, then. The kernel could change their handling of this status code at any time in the future. Especially since this particular status code doesn\u0027t mean what we want it to mean (it means the NAND went bad).",
      "parentUuid": "0a95eb2e_e8e6f8c0",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b5d6d764_9b9291d1",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 173,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2018-08-31T00:09:01Z",
      "side": 1,
      "message": "Agreed. This replies on how the initiator side handles the status. The problem here is that other status is not working and cause the \"nvme list\" \"nvme disconnect\" hanging at the initiator side and only reboot can help. So that I picked this status where the Subsystem is not ready to handle I/Os and somewhat to reflect the status of \"Access Denied\". \nOr we just keep it as original behavior to eventually timed out and let the Initiator side reset the controller which also looks like depends on how the Initiator handles.\n\nLet me know if there is any further suggestion.",
      "parentUuid": "b042a560_fdb7d35b",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3f269b5e_96018b6b",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 175,
      "author": {
        "id": 1011207
      },
      "writtenOn": "2018-08-29T05:39:39Z",
      "side": 1,
      "message": "When returning this error, the initiator side like kernel nvme host will display as following and handle this situation properly.\n\n[Wed Aug 29 13:16:54 2018] nvme nvme0: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme0: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode1\", addr 192.168.89.11:4420\n[Wed Aug 29 13:16:54 2018] nvme nvme1: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme1: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode2\", addr 192.168.89.11:4420\n[Wed Aug 29 13:16:54 2018] nvme nvme2: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme2: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode3\", addr 192.168.89.11:4420\n[Wed Aug 29 13:16:54 2018] nvme nvme3: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme3: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode4\", addr 192.168.89.11:4420\n[Wed Aug 29 13:16:54 2018] print_req_error: critical medium error, dev nvme3c33n1, sector 3125627392\n[Wed Aug 29 13:16:54 2018] print_req_error: critical medium error, dev nvme3c33n1, sector 3125627392\n[Wed Aug 29 13:16:54 2018] Buffer I/O error on dev nvme3n1, logical block 390703424, async page read\n[Wed Aug 29 13:16:54 2018] nvme nvme4: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme4: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode5\", addr 192.168.89.11:4420\n[Wed Aug 29 13:16:54 2018] nvme nvme5: creating 3 I/O queues.\n[Wed Aug 29 13:16:54 2018] nvme nvme5: new ctrl: NQN \"nqn.2016-06.io.spdk:cnode6\", addr 192.168.89.11:4420",
      "range": {
        "startLine": 174,
        "startChar": 0,
        "endLine": 175,
        "endChar": 61
      },
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "146e06d0_7600d43f",
        "filename": "lib/nvmf/request.c",
        "patchSetId": 3
      },
      "lineNbr": 182,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2018-08-30T17:01:03Z",
      "side": 1,
      "message": "You still want to queue incoming requests on the admin queue while the subsystem is paused or we\u0027ll end up with race conditions where two threads are modifying the subsystem state at the same time.",
      "revId": "512017a38e3b4c808b5f04a307075a31b4cc6369",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    }
  ]
}