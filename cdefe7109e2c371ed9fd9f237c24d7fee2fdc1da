{
  "comments": [
    {
      "key": {
        "uuid": "c16a9585_3c9862fc",
        "filename": "include/spdk/fc_adm_api.h",
        "patchSetId": 23
      },
      "lineNbr": 1,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "This isn\u0027t a header that corresponds to a publicly available API. It should either be in include/spdk_internal or in lib/nvmf.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "f3c26628_4512d4e8",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1458,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Who calls this function? Is the LLD supposed to? If so, we should pass the LLD a function pointer to this when it is initialized in order to not have circular dependencies.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "3710dd12_8eb44ae6",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "I am really struggling to see how there is no \"listen\" step - isn\u0027t there some configuration to the link services stuff that indicates which connections it should accept and which it shouldn\u0027t? This is the mechanism that should be passing that information to it.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "381bca7e_9a9ebc77",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T05:47:21Z",
      "side": 1,
      "message": "For each FC HBA,\nLLD knows WWNN shared by all ports of a device, and WWPNs which is unique to each port.\nFor each port, LLD calls nvmf_fc_adm_evnt_hw_port_init() by passing SPDK_FC_HW_PORT_INIT to admin API.\n\nEach FC port is described as a struct spdk_nvmf_fc_port.\nFC ports are registered into a global list g_spdk_nvmf_fc_port_list.\n\nEach FC port has a link service (LS) queue and multiple IO queues.\nThe LS queue is is registered into LLD as a special HWQP.\nThe HWQP of the LS queue almost corresponds to the port registered in the RDMA or TCP transport.\n\nIO queues are registered into LLD as HWQPs\n\nThe LS queue is affinitized into the master thread (this may match the thread of the acceptor poller.)\nIO queues are spread fairly among multiple threads.\n\nEach FC port holds multiple FC nports.\nFC nport holds multple FC remote ports.\n\nFC nport corresponds to NVMe-oF subsystem or iSCSI target.\nFC remote port corresponds to NVMe-oF initiator or iSCSI initiator.\n\n\nnvmf_fc_adm_hw_port_data_init(fc_port, args)\n{\n\tinitialize fc_port-\u003els_queue and register it into LLD.\n\tmultiple fc_port-\u003eio_queues and register them into LLD.\n\tinitialize nport list of fc_port.\n}\n\nnvmf_fc_adm_evnt_hw_port_init(api_data)\n{\n\talloc fc_port;\n\talloc fc_port-\u003eio_queues;\n\tnvmf_fc_adm_hw_port_data_init(fc_port, api_data);\n\tadd fc_port to the global list g_spdk_nvmf_fc_port_list.\n}\n\n\nIt will be helpful to understand if g_spdk_nvmf_fc_port_ist is in struct spdk_nvmf_fc_transport.",
      "parentUuid": "3710dd12_8eb44ae6",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "88086b73_f35d1912",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "This needs to call cb_fn for each new qpair it creates. That qpair will then get assigned to a poll group. I suspect this may be a problem for FC though. Can you walk me through what happens once the LS queue is polled and a new connection is discovered? It needs to get assigned to a hwqp or something, right?",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c1a0ed06_258342c9",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T07:31:07Z",
      "side": 1,
      "message": "Hi Anil,\nMy question is\n- if we assign FC connection to HWQP not when CASS but when CIOC, implementation of FC transport may become closer?\n\nBesides, I tried to explain the current implementation for Ben\u0027s question. Will you improve my bad explanation?\n \nCritical terms may be CASS (Create Association) and CIOC (Create I/O connection).\n\nCASS request is used to create a FC association.\n\nFC association is a relationship between an initiator remort port and a target nport.\nFC connection is an abstraction representing a qpair for an NVMe controller.\n\nA FC connection corresponding to the Admin Queue is created with simultaneously\nwith the creation of the FC association as part of CASS LS request.\n\nA FC connection corresponding to an I/O queue is created as part of processing CIOC.\n\n\nThe first NVMe-oF I/O operation issued on an FC connection is an NVMe-oF Connect command for the corresponding NVMe controller queue.\n\n\n\nstruct spdk_nvmf_fc_conn corresponds struct spdk_nvmf_rdma_qpair or spdk_nvmf_tcp_qpair.\n\n\nIn TCP transport, spdk_nvmf_tcp_port_accept is created in _spdk_nvmf_tcp_handle_connect and it is registered into NVMf poll group in its callback.\n\nOn the other hand, in FC transport,\n\nwhen creating a FC association by processing CASS LS request, spdk_nvmf_fc_conn pool of a FC association is created, and the spdk_nvmf_fc_conn for the admin queue is created by getting a conn from the pool, and\n\nwhen creating a FC connection by processing CIOC LS request, a spdk_nvmf_fc_conn for the IO queue is created by getting a conn from the pool.\n\n\nAll conns in the spdk_nvmf_fc_conn pool are assigned to a single HWQP at creation of the pool.\n\nHWQPs of a single FC port are assigned to cores when changing the FC port to online.",
      "parentUuid": "88086b73_f35d1912",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "17856c4f_c02a44d7",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1955,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Not thread safe",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b0a4a342_abcacce3",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1995,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "When you poll the hwqp, where does it come back up to this layer for each request?",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d2d8ade6_8aa5dc8f",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 23
      },
      "lineNbr": 190,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "In this FC implementation, the FC layer is allocating its own pool of bdev_io. That\u0027s backward from the SPDK model where the bdev layer owns all of the bdev_io objects. This will need to be reworked into the SPDK model to get merged.\n\nI\u0027m familiar with the history of why this is the way it is, but with the current SPDK bdev_io pooling strategy the original reasons for doing this are no longer valid. It\u0027s important architecturally to SPDK to manage the bdev_io in the bdev layer.\n\nThe transport should know nothing about the bdev layer.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4fe12bb2_a3870c7e",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 23
      },
      "lineNbr": 190,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Actually, it looks like the work as already been done, but these data members just haven\u0027t been dropped yet. I think you can just delete these 3 data members and you\u0027re good to go.",
      "parentUuid": "d2d8ade6_8aa5dc8f",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}