{
  "comments": [
    {
      "key": {
        "uuid": "c16a9585_3c9862fc",
        "filename": "include/spdk/fc_adm_api.h",
        "patchSetId": 23
      },
      "lineNbr": 1,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "This isn\u0027t a header that corresponds to a publicly available API. It should either be in include/spdk_internal or in lib/nvmf.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4b1ae390_6b419a69",
        "filename": "include/spdk/fc_adm_api.h",
        "patchSetId": 23
      },
      "lineNbr": 1,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T20:40:08Z",
      "side": 1,
      "message": "LLD driver uses this API to post notification such as link events, port online/offline, initiator disconnect, ABTS etc. back to FC-NVMe transport. Moving this to include/spdk_internal.",
      "parentUuid": "c16a9585_3c9862fc",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "f3c26628_4512d4e8",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1458,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Who calls this function? Is the LLD supposed to? If so, we should pass the LLD a function pointer to this when it is initialized in order to not have circular dependencies.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "7e927f36_853150c9",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1458,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T23:57:59Z",
      "side": 1,
      "message": "This gets called in poll context from LLD when it pulls a new command frame from receive queue. Don\u0027t think this will cause any circular dependency issue.",
      "parentUuid": "f3c26628_4512d4e8",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3710dd12_8eb44ae6",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "I am really struggling to see how there is no \"listen\" step - isn\u0027t there some configuration to the link services stuff that indicates which connections it should accept and which it shouldn\u0027t? This is the mechanism that should be passing that information to it.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "381bca7e_9a9ebc77",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T05:47:21Z",
      "side": 1,
      "message": "For each FC HBA,\nLLD knows WWNN shared by all ports of a device, and WWPNs which is unique to each port.\nFor each port, LLD calls nvmf_fc_adm_evnt_hw_port_init() by passing SPDK_FC_HW_PORT_INIT to admin API.\n\nEach FC port is described as a struct spdk_nvmf_fc_port.\nFC ports are registered into a global list g_spdk_nvmf_fc_port_list.\n\nEach FC port has a link service (LS) queue and multiple IO queues.\nThe LS queue is is registered into LLD as a special HWQP.\nThe HWQP of the LS queue almost corresponds to the port registered in the RDMA or TCP transport.\n\nIO queues are registered into LLD as HWQPs\n\nThe LS queue is affinitized into the master thread (this may match the thread of the acceptor poller.)\nIO queues are spread fairly among multiple threads.\n\nEach FC port holds multiple FC nports.\nFC nport holds multple FC remote ports.\n\nFC nport corresponds to NVMe-oF subsystem or iSCSI target.\nFC remote port corresponds to NVMe-oF initiator or iSCSI initiator.\n\n\nnvmf_fc_adm_hw_port_data_init(fc_port, args)\n{\n\tinitialize fc_port-\u003els_queue and register it into LLD.\n\tmultiple fc_port-\u003eio_queues and register them into LLD.\n\tinitialize nport list of fc_port.\n}\n\nnvmf_fc_adm_evnt_hw_port_init(api_data)\n{\n\talloc fc_port;\n\talloc fc_port-\u003eio_queues;\n\tnvmf_fc_adm_hw_port_data_init(fc_port, api_data);\n\tadd fc_port to the global list g_spdk_nvmf_fc_port_list.\n}\n\n\nIt will be helpful to understand if g_spdk_nvmf_fc_port_ist is in struct spdk_nvmf_fc_transport.",
      "parentUuid": "3710dd12_8eb44ae6",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "18885cb0_45b4db9e",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T20:40:08Z",
      "side": 1,
      "message": "Shuhei, your understanding is correct. Are you suggesting to add spdk_nvmf_fc_port_list to spdk_nvmf_fc_transport structure?",
      "parentUuid": "381bca7e_9a9ebc77",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "82aaa6ed_e8aea404",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T23:13:42Z",
      "side": 1,
      "message": "Anil, yes, if reasonable from your viewpoint.",
      "parentUuid": "18885cb0_45b4db9e",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "513a50a9_64e25c83",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T23:57:59Z",
      "side": 1,
      "message": "Ben, there is no need to have server side listening for FC-NVMe. We can try implement WWNN/WWPN based listening to match RDMA \u0026 TCP but it is not necessary. The current implementation would satisfy most FC vendor/customer requirements.",
      "parentUuid": "18885cb0_45b4db9e",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "2067a0a6_0d7ae7ed",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1892,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-23T00:22:08Z",
      "side": 1,
      "message": "Anil, may I interrupt? As long as I talked with my colleagues, it\u0027s very nice to have the feature to control the ports each NVMf subsystem listens. I don\u0027t know this is general, but other FC vendor/customers may have similar request.",
      "parentUuid": "513a50a9_64e25c83",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "88086b73_f35d1912",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "This needs to call cb_fn for each new qpair it creates. That qpair will then get assigned to a poll group. I suspect this may be a problem for FC though. Can you walk me through what happens once the LS queue is polled and a new connection is discovered? It needs to get assigned to a hwqp or something, right?",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c1a0ed06_258342c9",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T07:31:07Z",
      "side": 1,
      "message": "Hi Anil,\nMy question is\n- if we assign FC connection to HWQP not when CASS but when CIOC, implementation of FC transport may become closer?\n\nBesides, I tried to explain the current implementation for Ben\u0027s question. Will you improve my bad explanation?\n \nCritical terms may be CASS (Create Association) and CIOC (Create I/O connection).\n\nCASS request is used to create a FC association.\n\nFC association is a relationship between an initiator remort port and a target nport.\nFC connection is an abstraction representing a qpair for an NVMe controller.\n\nA FC connection corresponding to the Admin Queue is created with simultaneously\nwith the creation of the FC association as part of CASS LS request.\n\nA FC connection corresponding to an I/O queue is created as part of processing CIOC.\n\n\nThe first NVMe-oF I/O operation issued on an FC connection is an NVMe-oF Connect command for the corresponding NVMe controller queue.\n\n\n\nstruct spdk_nvmf_fc_conn corresponds struct spdk_nvmf_rdma_qpair or spdk_nvmf_tcp_qpair.\n\n\nIn TCP transport, spdk_nvmf_tcp_port_accept is created in _spdk_nvmf_tcp_handle_connect and it is registered into NVMf poll group in its callback.\n\nOn the other hand, in FC transport,\n\nwhen creating a FC association by processing CASS LS request, spdk_nvmf_fc_conn pool of a FC association is created, and the spdk_nvmf_fc_conn for the admin queue is created by getting a conn from the pool, and\n\nwhen creating a FC connection by processing CIOC LS request, a spdk_nvmf_fc_conn for the IO queue is created by getting a conn from the pool.\n\n\nAll conns in the spdk_nvmf_fc_conn pool are assigned to a single HWQP at creation of the pool.\n\nHWQPs of a single FC port are assigned to cores when changing the FC port to online.",
      "parentUuid": "88086b73_f35d1912",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "51fb1ad8_44faee99",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T20:40:08Z",
      "side": 1,
      "message": "That is exactly what happens during LS queue polling. Here are the 3 relevant flows,\n\nController Admin Queue setup:\nspdk_nvmf_fc_hwqp_process_pending_ls_rqsts() --\u003e spdk_nvmf_fc_handle_ls_rqst() --\u003e nvmf_fc_ls_process_cass() --\u003e nvmf_fc_ls_add_conn_to_poller(AQ connection)\n\nIO Queue setup:\nspdk_nvmf_fc_hwqp_process_pending_ls_rqsts() --\u003e spdk_nvmf_fc_handle_ls_rqst() --\u003e nvmf_fc_ls_process_cioc() --\u003e nvmf_fc_ls_add_conn_to_poller(IOQ connection)\n\nIO Queue teardown:\nspdk_nvmf_fc_hwqp_process_pending_ls_rqsts() --\u003e spdk_nvmf_fc_handle_ls_rqst() --\u003e nvmf_fc_ls_process_disc() --\u003e nvmf_fc_ls_disconnect_assoc() --\u003e nvmf_fc_delete_association() --\u003e foreach connection { spdk_nvmf_fc_poller_api_func(SPDK_NVMF_FC_POLLER_API_DEL_CONNECTION) }",
      "parentUuid": "88086b73_f35d1912",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "79720d57_ac199a02",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1917,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T23:57:59Z",
      "side": 1,
      "message": "shuhei, you are right, CIOC creates IOQ connections only. AQ connections are implicitly created upon successful CASS. Yes, HWQP\u0027s are uniformly distributed among available cores. When target accepts a new connection, transport will scan hwqp\u0027s and assigns the new connection to the one with highest number of available command slots.",
      "parentUuid": "51fb1ad8_44faee99",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "17856c4f_c02a44d7",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1955,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Not thread safe",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9b68c892_506e16b4",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1955,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T23:57:59Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "17856c4f_c02a44d7",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "b0a4a342_abcacce3",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1995,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "When you poll the hwqp, where does it come back up to this layer for each request?",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "22be13f7_5a49c19c",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1995,
      "author": {
        "id": 1012251
      },
      "writtenOn": "2019-05-22T07:46:43Z",
      "side": 1,
      "message": "Hi Anil, related with this, will you clarify what APIs are called by LLD especially when polling the hwqp?",
      "parentUuid": "b0a4a342_abcacce3",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b5b8d8bc_7b6ec6fc",
        "filename": "lib/nvmf/fc.c",
        "patchSetId": 23
      },
      "lineNbr": 1995,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T23:57:59Z",
      "side": 1,
      "message": "LLD would handover few request frame to FC transport in spdk_nvmf_fc_hwqp_process_frame() call.",
      "parentUuid": "22be13f7_5a49c19c",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d2d8ade6_8aa5dc8f",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 23
      },
      "lineNbr": 190,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:40:06Z",
      "side": 1,
      "message": "In this FC implementation, the FC layer is allocating its own pool of bdev_io. That\u0027s backward from the SPDK model where the bdev layer owns all of the bdev_io objects. This will need to be reworked into the SPDK model to get merged.\n\nI\u0027m familiar with the history of why this is the way it is, but with the current SPDK bdev_io pooling strategy the original reasons for doing this are no longer valid. It\u0027s important architecturally to SPDK to manage the bdev_io in the bdev layer.\n\nThe transport should know nothing about the bdev layer.",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4fe12bb2_a3870c7e",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 23
      },
      "lineNbr": 190,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-05-21T19:53:01Z",
      "side": 1,
      "message": "Actually, it looks like the work as already been done, but these data members just haven\u0027t been dropped yet. I think you can just delete these 3 data members and you\u0027re good to go.",
      "parentUuid": "d2d8ade6_8aa5dc8f",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "94a955b8_673616dc",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 23
      },
      "lineNbr": 190,
      "author": {
        "id": 1015300
      },
      "writtenOn": "2019-05-22T20:40:08Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "4fe12bb2_a3870c7e",
      "revId": "cdefe7109e2c371ed9fd9f237c24d7fee2fdc1da",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    }
  ]
}