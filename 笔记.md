# 创建target的工作
1. 初始化tgt中transports的队列。
2. spdk_io_device_register是重点。
3. spdk_nvmf_tgt_create_poll_group： 
            初始化tgroups和qpairs。
            将tgt中的所有transport加入到spdk_nvmf_poll_group中：
                          如果在这个spdk_nvmf_poll_group中的spdk_nvmf_transport_poll_group队列中已经存在该transport，则退出。
                          为该transport创建一个spdk_nvmf_transport_poll_group：
                                              调用具体协议的poll_group_create为该transport创建一个spdk_nvmf_transport_poll_group。
                                              将该transport赋值给spdk_nvmf_transport_poll_group这个实体的transport。
                                              初始化spdk_nvmf_transport_poll_group的buf_cache队列。
                                              获取buf
                          将spdk_nvmf_poll_group赋值给spdk_nvmf_transport_poll_group的group。
            注册spdk_nvmf_poll_group的poll函数：对于这个spdk_nvmf_poll_group中的所有spdk_nvmf_transport_poll_group调用poll，进而调用具体协议的poll函数。 


# 创建transport
1. 调用底层的create方法：对于每一个tcp_transport, 初始化一个port的队列。
2. 申请transport的data_buf_pool.
3. spdk_nvmf_tgt_add_transport：将新创建的transport加入到tgt的transport队列中。
    3.1：_spdk_nvmf_tgt_add_transport ：将transport加入到spdk_nvmf_poll_group中 
        3.1.1：spdk_nvmf_poll_group_add_transport：创建一个该transport的poll_group,里面创建一个sockgroup，有pending_data_buf_queue和qpairs的队列。


# 我要做的
1. 创建一个ftp_server，将他添加到spdk_ftp_tgt中，同时创建一个ftp的poolgroup,在该poolgroup中创建一个sockgroup，需要有pending_data_buf_queue和qpairs队列吗


# 对transport进行监听
nvmf.c: spdk_nvmf_transport_listen
调用底层tcp的listen
1. 创建一个port，为该port的listen_sock进行监听，将该port插入到该transport的port队列中。
2. 为什么需要锁？

## 我要做的事情
1. 调用spdk_sock_listen

# accept_poll
1. spdk_nvmf_tgt_accept:对于tgt里面的每个transport进行accept。以调用spdk_nvmf_tcp_accept为例
2. spdk_nvmf_tcp_accept： 对于该transport中的每个port调用spdk_nvmf_tcp_port_accept
            2.1 spdk_nvmf_tcp_port_accept：每次可以处理多个accept请求。对listen_sock调用accept，相当于socket编程中的accept，然后调用_spdk_nvmf_tcp_handle_connect
            2.2 _spdk_nvmf_tcp_handle_connect：申请一个新的qpair，根据sock信息填充qpair的信息，包括server和client的地址和端口，回调函数传输qpair。
3. 回调函数：获取nvmf_tgt_poll_group中的一个pollgroup，将这个qpair添加到该poolgroup中。
            一个spdk_nvmf_poll_group有多种spdk_nvmf_transport_poll_group，将该qpair添加到对应的poolgroup中。
            spdk_nvmf_tcp_poll_group_add： 将这个qpair的sock添加到该poolgroup的sockgroup中。 TAILQ_INIT(&qpair->outstanding)；干什么用的
                                            回调函数中内容很多。
                                            将该qpair插入到该poolgroup的qpair队列中。
3. 回调函数：
            spdk_nvmf_poll_group_add（spdk_nvmf_poll_group，spdk_nvmf_qpair):
                      3.1. 初始化新连接的spdk_nvmf_request队列
                      3.2. 将该spdk_nvmf_poll_group复制给qpair的group
                      3.3. 将qpair添加到该spdk_nvmf_poll_group中spdk_nvmf_transport_poll_group队列 中与该qpair的transport相同的spdk_nvmf_transport_poll_group中。spdk_nvmf_transport_poll_group_add(spdk_nvmf_transport_poll_group,spdk_nvmf_qpair):
                             3.3.1: 确保该qpair的tansport与该group的transport相同。
                             2



所做的事情：
对于tgt里面的每个ftp_server进行遍历，调用accept函数，对server的listen_sock调用spdk_sock_accept函数，对得到的sock调用_spdk_ftp_handle_connect, 创建一个新的结构体spdk_ftp_tcp_conn,使用spdk_sock_getaddr得到这个结构体的ftp的地址和端口，调用回调函数。
回调函数：在ftp_tgt_pool_group队列中找到一个pollgroup，将该conn添加到该pool_group中.
            将该conn的sock添加到该pool_group的sockgroup中，然后调用回调函数。对conn的参数进行初始化，将该conn插入到该poolgroup中。
            sock添加到sockgroup中的回调函数：调用spdk_nvmf_tcp_sock_process对该conn进行处理。


# 各种类型的poll_group
1. spdk_nvmf_poll_group
2. spdk_nvmf_transport_poll_group
3. spdk_nvmf_tcp_poll_group
4. 



# 留的坑
1. ftp_tcp.c中在spdk_ftp_tcp_poll_group_create中，创建的时候没有初始化qpairs和pending_data_buf_queue。
2. ftp_server.c spdk_ftp_server_poll_group_create中初始化buf_cache，并进行申请buf都没有做。
3. ftp.c 中spdk_ftp_tgt_add_server没有将server添加到spdk_ftp_poll_group中。
4. ftp_tcp.c中_spdk_ftp_tcp_handle_connect没有初始化全参数。
5. ftp_tcp.c中spdk_ftp_tcp_conn_destroy中spdk_ftp_tcp_conn_destroy中没有释放资源。
5. spdk_ftp_conn_disconnect没有实现。
6. new_conn 里面的else没有处理。
7. ftp_tcp.c中spdk_ftp_tcp_sock_cb中没有检查退出的情况。
8. spdk_ftp_tcp_sock_process没有实现。
9. spdk_ftp_poll_group_add中没有设置conn的state。



# 遇到的问题：
1. 将server添加到tgt时出错。
2. spdk_nvmf_tgt_create中的spdk_io_device_register是什么意思。spdk_nvmf_poll_group_create中的spdk_get_io_channel是什么意思。
3. 各个pool_group之间的关系。
4. >这个new_qpair函数在正常情况下(这里忽略一些一些异常处理的case)， 会选择一个CPU core，算法目前采用的是round-robin，然后通过thread之间传播时间的函数spdk_event_allocate， 使得这个core执行nvmf_tgt_poll_group_add。    

创建的时候怎么知道他在哪个core上？
5. > 前面我们提到每个thread上都会创建一个定时器来运行spdk_nvmf_poll_group_poll。 在这个函数中，我们会调用这个transport对应的polling 函数。

每个spdk_nvmf_poll_group就是一个线程吗？
4. I/Ochannel的概念。
5. blobfs多核读写文件是否可行
5. 多文件打开读写。权限问题。
6. 使用几个核来完成它
7. 抽象问题：server是否对应nvmf中的transport？一个server的connection是否对应于一个tranposrt的qpair？在nvmf.c中spdk_nvmf_tgt_create_poll_group中，。。。。
8. nvmf.c中spdk_nvmf_poll_group_add_transport检测到transport可能存在于group中，这个spdk_nvmf_tgt_create_poll_group函数会在什么时候被调用？
9. 为什么要在监听的时候加锁？


# accpet_poll
1. spdk_nvmf_tgt_accept: 对于tgt里面的每个transport



# 所作的工作
1. spdk框架的初始化工作，创建了spdk_ftp_tgt,spdk_ftp_poll_group,spdk_ftp_server_poll_group这些东西的初始化动作。
2. 模仿nvmf，抽象出底层协议的具体操作，以便后期多种协议的接入。
3. 解析conf文件进行监听，listensock
4. 在将新建的server添加到tgt中的pollgroup中时发生错误。





