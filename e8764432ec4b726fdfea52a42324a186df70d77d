{
  "comments": [
    {
      "key": {
        "uuid": "1d757d4c_d44eccbb",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 343,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-09-16T20:16:18Z",
      "side": 1,
      "message": "Does rdma_get_cm_event guarantee that you won\u0027t get two events for the same qpair without an ack in the middle? If it doesn\u0027t, here\u0027s a crazy scenario. Say you have 3 events for the same queue pair. The first one comes in and gets assigned to event_qpair-\u003eevt. The second one is then processed and gets queued on the controller. While the second one is processing, the other thread handles the first event and sets evt back to NULL. Then the third one ends up skipping in line ahead of the second one.\n\nIs this something we need to worry about or handle?",
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "7a8a610b_f47e21ef",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 343,
      "author": {
        "id": 1011223
      },
      "writtenOn": "2019-09-16T21:19:18Z",
      "side": 1,
      "message": "I\u0027m thinking that\u0027s something we don\u0027t have to worry about.\n\nNot because it can\u0027t happen - I actually think it\u0027s possible that we could get three events in a row like that. But right now, the only events we do anything in response to are connect/disconnect events (which are processed synchronously under the lock so no racing there) and the disconnected and device removal events. Since there are really only 2 events (at the moment) that we are responding to on a separate thread, we won\u0027t really hit the three events in a row problem.\n\nWhen we get to the point where we are processing some of the other events in a more sophisticated way, there will be a couple things that we need to address:\n1. Disconnecting / reconnecting an rdma qpair - make sure that we properly ack all events before destroying the rqpair, even those still in the controller list.\n2. Getting bursts of events - I think this will only happen in really bad error cases, and we will only want to respond to the first event rather than initializing two or three recovery mechanisms in a row. I don\u0027t think that we will have very much recourse when we get an error than disconnecting and reconnecting the qpair so we can just keep a flag indicating that that\u0027s what we\u0027re doing. That will pretty much solve the problem of getting three or more events in a row and racing between the two threads to respond in the proper order to all of them.",
      "parentUuid": "1d757d4c_d44eccbb",
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2a11dd05_d7950f3d",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 1627,
      "author": {
        "id": 1016482
      },
      "writtenOn": "2019-09-13T11:26:34Z",
      "side": 1,
      "message": "Could you remove extra empty lines?",
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e6d49767_e3637af9",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 1627,
      "author": {
        "id": 1011223
      },
      "writtenOn": "2019-09-17T18:30:26Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "2a11dd05_d7950f3d",
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "db7d2fed_d9084656",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 1679,
      "author": {
        "id": 1016482
      },
      "writtenOn": "2019-09-13T11:26:34Z",
      "side": 1,
      "message": "It seems that there is no need to remove entries from lists since the memory occupied by entries is freed on line 1681",
      "range": {
        "startLine": 1674,
        "startChar": 0,
        "endLine": 1679,
        "endChar": 2
      },
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "402973e7_06b5bd00",
        "filename": "lib/nvme/nvme_rdma.c",
        "patchSetId": 12
      },
      "lineNbr": 1679,
      "author": {
        "id": 1011223
      },
      "writtenOn": "2019-09-17T18:30:26Z",
      "side": 1,
      "message": "True, but I feel better removing the entries from any data structures they are in before freeing them just to keep anyone from accidentally using them later. For example in the implementation of nvme_ctrlr_destruct_finish, etc.\nI agree that in the current implementation, we are going to try to touch the stailqs before the rctrlr structure is freed, but I feel safer knowing that even if a future implementation messed that up, they wouldn\u0027t be able to try to access freed memory.\nPlus, it\u0027s not in the data path, so I\u0027m not worried about the small performance impact of emptying the queues.",
      "parentUuid": "db7d2fed_d9084656",
      "range": {
        "startLine": 1674,
        "startChar": 0,
        "endLine": 1679,
        "endChar": 2
      },
      "revId": "e8764432ec4b726fdfea52a42324a186df70d77d",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}