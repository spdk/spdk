{
  "comments": [
    {
      "key": {
        "uuid": "5fe253f1_9619887b",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-21T08:46:38Z",
      "side": 1,
      "message": "Doing this will get us to amortize the cost of the system call. But we will have twice copies (from kernel buffer to pipe and from pipe to the spdk nvme/nvmf buffer that goes to the bdev).\n\nThis approach might turn to be problematic from few aspects:\n\nThe SPDK approach is zero copy on data-path. For TCP we have now one copy (user buffer \u003c--\u003e kernel socket buffer) and we\u0027re aiming to get to zero copy (for TX your MSG_ZEROCOPY patches and some TBD work/thinking we have for RX). \n\nThis work gets us to have two copies and not one copy in some (many cases) which is @ high level against where we want the wind to blow to.\n\nOn systems such as SoC ARM based, where the copy cost is notable increasing it is going to hurt the performance notably. On such systems we should get to 0-copy and nothing more..\n\nI gave the patches quick test and the cpu cost for copy + syscall was higher with the patch (upper listing) vs. without it (lower listing, just used the same code without the top most patch). \n\nThis was nvme-perf with write test, six initiator threads pushing to one target thread, depth 64, size 4k, all goes in-capsule). \n\npipe cost of syscall + copy \u003d 32% (14.1 + 11.8 + 3 + 2.58)\n  14.19%  [kernel]            [k] copy_user_enhanced_fast_string\n  11.84%  libc-2.17.so        [.] __memcpy_ssse3_back\n   5.26%  nvmf_tgt            [.] spdk_nvmf_tcp_req_set_state\n   3.36%  nvmf_tgt            [.] spdk_nvmf_tcp_req_process\n   3.07%  [kernel]            [k] entry_SYSCALL_64\n   2.58%  [kernel]            [k] syscall_return_via_sysret\n   2.23%  [kernel]            [k] skb_release_data\n\nno pipe cost of syscall + copy \u003d 23.5% (8.7 + 8.5 + 7.2)\n   8.70%  [kernel]            [k] entry_SYSCALL_64\n   8.45%  [kernel]            [k] copy_user_enhanced_fast_string\n   7.24%  [kernel]            [k] syscall_return_via_sysret\n   4.39%  nvmf_tgt            [.] spdk_nvmf_tcp_req_set_state\n   3.73%  [kernel]            [k] __skb_datagram_iter\n   2.43%  nvmf_tgt            [.] spdk_nvmf_tcp_req_process",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2271ddb6_a1cebb97",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-21T08:53:02Z",
      "side": 1,
      "message": "some more data on the perf traces: it\u0027s from x86 (Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz) and RHEL7.x kernels.",
      "parentUuid": "5fe253f1_9619887b",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d424c8c9_64748bcb",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-11-21T16:45:33Z",
      "side": 1,
      "message": "Did you also compare the I/Ops between the two patches? We see a very significant performance improvement using this buffering strategy. Up to 50% improvements on random write workloads, for example. My numbers are from an older version of this patch, so we\u0027ll make sure to re-run the benchmarks on the latest version.\n\nNote that the current NVMe-oF target already does this buffering. This patch series is just pushing it down into the posix sock layer instead, so that when there are other sock layers available that don\u0027t benefit from buffering they aren\u0027t forced to do it.",
      "parentUuid": "2271ddb6_a1cebb97",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8378bcd6_5a0e7d7e",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-24T18:00:10Z",
      "side": 1,
      "message": "Unfortunately.. I wasn\u0027t aware that for nvmf a 2nd copy is already upstream (just came across Aug 12th commit d50736776c94d \"nvmf/tcp: Use a big buffer for PDU receving\" today. Now, if this series just pushes the copy down into the posix sock layer, I don\u0027t understand why do you see 50% improvement? can you explain that?\n\nYeah, on x86 I saw some improvement with this series, I still can\u0027t explain it, need to get deeper into the code. From the raw cpu %% I listed above it\u0027s also not clear why we get better IOPS.",
      "parentUuid": "d424c8c9_64748bcb",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2971f43f_07bc5332",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-11-25T16:46:42Z",
      "side": 1,
      "message": "This series doesn\u0027t give the 50% improvement - the original patch (d5073677) did. This series is just moving around where the buffering is done. The performance should remain unchanged (on x86) when comparing this series to current master. If it doesn\u0027t, then I haven\u0027t done this patch correctly.",
      "parentUuid": "8378bcd6_5a0e7d7e",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "88551d55_2704216d",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-26T13:13:14Z",
      "side": 1,
      "message": "From what I saw the target side perf CPU %% accounting for (syscall + copy) before and after the patch are notably different - so something happens here. \n\nNote that since you internalized the change in the posix code, it comes into play also on the initiator side, unlike the earlier change which only affects the target -- both nvme/nvmf call  nvme_tcp_read_data() which further invokes spdk_sock_recv() which goes to the posix code you modified.",
      "parentUuid": "2971f43f_07bc5332",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "865982fd_26167a46",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 192,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "instead of adding one byte, should we round it up to the next cache line?  otherwise we only touch 1 byte in that last cache line which seems like a tiny waste (does it matter if the sz is exact?)",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "14a3be6e_f45abb08",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 192,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-12-16T16:46:36Z",
      "side": 1,
      "message": "The amount of data available in the pipe is one less than what was provided in the buffer. I\u0027ll round up.",
      "parentUuid": "865982fd_26167a46",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c4ff49db_2a1f8728",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 207,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "would it be cleaner to do this before you even bother to allocate new_buf/new_pipe?  (I\u0027m not sure - just a thought)",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b5c80792_959a1afb",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 218,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "I totally missed this when we did the spdk_iovcpy review - but should we order these as diov and then siov to match memcpy semantics?  I know that\u0027s a separate patch...",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "43d4ad4f_2d07a00a",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 232,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-26T13:06:04Z",
      "side": 1,
      "message": "nit, this is a point cleanup that doesn\u0027t belong to this patch (please..)",
      "range": {
        "startLine": 232,
        "startChar": 0,
        "endLine": 232,
        "endChar": 10
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ba865679_89a27574",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 240,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "if sz \u003c SO_RCVBUF_SIZE, do you want the pipe to be size sz even though we\u0027re going to bump sz up to SO_RCVBUF_SIZE just afterwards?  it wasn\u0027t clear to me whether that was intended or not",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b0e009f5_717d4155",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 252,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-26T13:06:04Z",
      "side": 1,
      "message": "nit, same as above",
      "range": {
        "startLine": 251,
        "startChar": 2,
        "endLine": 252,
        "endChar": 17
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "eec4f666_d71b83ff",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 297,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-26T13:06:04Z",
      "side": 1,
      "message": "nit, same (can do all the cleanups in one pre-patch to this one)",
      "range": {
        "startLine": 295,
        "startChar": 2,
        "endLine": 297,
        "endChar": 14
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}