{
  "comments": [
    {
      "key": {
        "uuid": "5fe253f1_9619887b",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-21T08:46:38Z",
      "side": 1,
      "message": "Doing this will get us to amortize the cost of the system call. But we will have twice copies (from kernel buffer to pipe and from pipe to the spdk nvme/nvmf buffer that goes to the bdev).\n\nThis approach might turn to be problematic from few aspects:\n\nThe SPDK approach is zero copy on data-path. For TCP we have now one copy (user buffer \u003c--\u003e kernel socket buffer) and we\u0027re aiming to get to zero copy (for TX your MSG_ZEROCOPY patches and some TBD work/thinking we have for RX). \n\nThis work gets us to have two copies and not one copy in some (many cases) which is @ high level against where we want the wind to blow to.\n\nOn systems such as SoC ARM based, where the copy cost is notable increasing it is going to hurt the performance notably. On such systems we should get to 0-copy and nothing more..\n\nI gave the patches quick test and the cpu cost for copy + syscall was higher with the patch (upper listing) vs. without it (lower listing, just used the same code without the top most patch). \n\nThis was nvme-perf with write test, six initiator threads pushing to one target thread, depth 64, size 4k, all goes in-capsule). \n\npipe cost of syscall + copy \u003d 32% (14.1 + 11.8 + 3 + 2.58)\n  14.19%  [kernel]            [k] copy_user_enhanced_fast_string\n  11.84%  libc-2.17.so        [.] __memcpy_ssse3_back\n   5.26%  nvmf_tgt            [.] spdk_nvmf_tcp_req_set_state\n   3.36%  nvmf_tgt            [.] spdk_nvmf_tcp_req_process\n   3.07%  [kernel]            [k] entry_SYSCALL_64\n   2.58%  [kernel]            [k] syscall_return_via_sysret\n   2.23%  [kernel]            [k] skb_release_data\n\nno pipe cost of syscall + copy \u003d 23.5% (8.7 + 8.5 + 7.2)\n   8.70%  [kernel]            [k] entry_SYSCALL_64\n   8.45%  [kernel]            [k] copy_user_enhanced_fast_string\n   7.24%  [kernel]            [k] syscall_return_via_sysret\n   4.39%  nvmf_tgt            [.] spdk_nvmf_tcp_req_set_state\n   3.73%  [kernel]            [k] __skb_datagram_iter\n   2.43%  nvmf_tgt            [.] spdk_nvmf_tcp_req_process",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2271ddb6_a1cebb97",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-11-21T08:53:02Z",
      "side": 1,
      "message": "some more data on the perf traces: it\u0027s from x86 (Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz) and RHEL7.x kernels.",
      "parentUuid": "5fe253f1_9619887b",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "d424c8c9_64748bcb",
        "filename": "/COMMIT_MSG",
        "patchSetId": 7
      },
      "lineNbr": 9,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-11-21T16:45:33Z",
      "side": 1,
      "message": "Did you also compare the I/Ops between the two patches? We see a very significant performance improvement using this buffering strategy. Up to 50% improvements on random write workloads, for example. My numbers are from an older version of this patch, so we\u0027ll make sure to re-run the benchmarks on the latest version.\n\nNote that the current NVMe-oF target already does this buffering. This patch series is just pushing it down into the posix sock layer instead, so that when there are other sock layers available that don\u0027t benefit from buffering they aren\u0027t forced to do it.",
      "parentUuid": "2271ddb6_a1cebb97",
      "range": {
        "startLine": 9,
        "startChar": 0,
        "endLine": 9,
        "endChar": 54
      },
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "865982fd_26167a46",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 192,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "instead of adding one byte, should we round it up to the next cache line?  otherwise we only touch 1 byte in that last cache line which seems like a tiny waste (does it matter if the sz is exact?)",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c4ff49db_2a1f8728",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 207,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "would it be cleaner to do this before you even bother to allocate new_buf/new_pipe?  (I\u0027m not sure - just a thought)",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b5c80792_959a1afb",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 218,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "I totally missed this when we did the spdk_iovcpy review - but should we order these as diov and then siov to match memcpy semantics?  I know that\u0027s a separate patch...",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ba865679_89a27574",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 7
      },
      "lineNbr": 240,
      "author": {
        "id": 1011222
      },
      "writtenOn": "2019-11-21T00:45:27Z",
      "side": 1,
      "message": "if sz \u003c SO_RCVBUF_SIZE, do you want the pipe to be size sz even though we\u0027re going to bump sz up to SO_RCVBUF_SIZE just afterwards?  it wasn\u0027t clear to me whether that was intended or not",
      "revId": "0db46b1484b69189152de6da23b2a65eb1832d46",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}