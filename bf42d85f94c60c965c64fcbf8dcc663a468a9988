{
  "comments": [
    {
      "key": {
        "uuid": "c50029f2_5d18b850",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-17T08:16:01Z",
      "side": 1,
      "message": "running perf with the zcopy patch applied on the merged writev_async bits [1] against null target with single thread, for queue depth 32 or higher, I get a flood of these prints:\n\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x2bb4ee0\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x2bb4ee0\n\nthe service doesn\u0027t stop, if I force zcopy to be off in posix.c - it doesn\u0027t happen. \n\n[1] commit listing\n\nae17a3d69 (HEAD -\u003e master) sock/posix: Zero copy send\nfeb769730 nvmf/tcp: Allocate pdu pool out of hugepages\nf59ec3a24 nvmf/tcp: Avoid long delays when target is exiting and there are idle connections\nb9395621a (origin/master, origin/HEAD) test/vhost: restore possibility to use custom Qemu binaries",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c9cc1e13_0bb1f998",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-17T09:20:33Z",
      "side": 1,
      "message": "I added accounting for the number of queued (writev_async) and pending (zcopy) requests, and dumped that on the writev async flush and zcopy completion handling -- for some reason the pending list goes way above the queue depth. \n\nThis doesn\u0027t make sense to me -- it was a 4KB read test with QD\u003d32 and the C2H success optimization was set as always (by default) -- so for each IO we expect one item from the target side to be on the pending list -- do you agree?\n\n\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x3300f00\n_sock_flush 26 queued reqs\n_sock_check_zcopy 64 pending reqs range [12483..12485]\n_sock_flush 6 queued reqs\n_sock_check_zcopy 32 pending reqs range [12486..12486]\n_sock_flush 26 queued reqs\n_sock_check_zcopy 32 pending reqs range [12487..12487]\n_sock_flush 6 queued reqs\n_sock_check_zcopy 32 pending reqs range [12488..12489]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\n_sock_check_zcopy 39 pending reqs range [12490..12491]\n_sock_flush 32 queued reqs\n_sock_flush 30 queued reqs\n_sock_check_zcopy 62 pending reqs range [12492..12492]\n_sock_flush 2 queued reqs\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12493..12495]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12496..12497]\n_sock_flush 32 queued reqs\n_sock_flush 15 queued reqs\n_sock_check_zcopy 47 pending reqs range [12498..12499]\n_sock_flush 32 queued reqs\n_sock_check_zcopy 32 pending reqs range [12500..12500]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12501..12502]\n_sock_flush 9 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 55 pending reqs range [12503..12504]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12505..12506]\n_sock_flush 32 queued reqs\n_sock_flush 32 queued reqs\n_sock_check_zcopy 64 pending reqs range [12507..12507]\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12508..12509]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12510..12511]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x3300f00\n_sock_flush 25 queued reqs\n_sock_check_zcopy 64 pending reqs range [12512..12513]\n_sock_flush 7 queued reqs\n_sock_check_zcopy 32 pending reqs range [12514..12514]\n_sock_flush 25 queued reqs",
      "parentUuid": "c50029f2_5d18b850",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "83fd4068_433ca5b1",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 12
      },
      "lineNbr": 543,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-12T20:06:03Z",
      "side": 1,
      "message": "So what about the crash I came across on earlier version [1], is it gone? or maybe what does this assert serve now when req-\u003einternal.offset is actually the system call index (and 0 is valid value)?\n\n[1] https://review.gerrithub.io/c/spdk/spdk/+/471752/9/module/sock/posix/posix.c#577",
      "range": {
        "startLine": 543,
        "startChar": 3,
        "endLine": 543,
        "endChar": 23
      },
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}