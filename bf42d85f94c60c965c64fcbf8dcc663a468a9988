{
  "comments": [
    {
      "key": {
        "uuid": "c50029f2_5d18b850",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-17T08:16:01Z",
      "side": 1,
      "message": "running perf with the zcopy patch applied on the merged writev_async bits [1] against null target with single thread, for queue depth 32 or higher, I get a flood of these prints:\n\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x2bb4ee0\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x2bb4ee0\n\nthe service doesn\u0027t stop, if I force zcopy to be off in posix.c - it doesn\u0027t happen. \n\n[1] commit listing\n\nae17a3d69 (HEAD -\u003e master) sock/posix: Zero copy send\nfeb769730 nvmf/tcp: Allocate pdu pool out of hugepages\nf59ec3a24 nvmf/tcp: Avoid long delays when target is exiting and there are idle connections\nb9395621a (origin/master, origin/HEAD) test/vhost: restore possibility to use custom Qemu binaries",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "c9cc1e13_0bb1f998",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-17T09:20:33Z",
      "side": 1,
      "message": "I added accounting for the number of queued (writev_async) and pending (zcopy) requests, and dumped that on the writev async flush and zcopy completion handling -- for some reason the pending list goes way above the queue depth. \n\nThis doesn\u0027t make sense to me -- it was a 4KB read test with QD\u003d32 and the C2H success optimization was set as always (by default) -- so for each IO we expect one item from the target side to be on the pending list -- do you agree?\n\n\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x3300f00\n_sock_flush 26 queued reqs\n_sock_check_zcopy 64 pending reqs range [12483..12485]\n_sock_flush 6 queued reqs\n_sock_check_zcopy 32 pending reqs range [12486..12486]\n_sock_flush 26 queued reqs\n_sock_check_zcopy 32 pending reqs range [12487..12487]\n_sock_flush 6 queued reqs\n_sock_check_zcopy 32 pending reqs range [12488..12489]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\n_sock_check_zcopy 39 pending reqs range [12490..12491]\n_sock_flush 32 queued reqs\n_sock_flush 30 queued reqs\n_sock_check_zcopy 62 pending reqs range [12492..12492]\n_sock_flush 2 queued reqs\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12493..12495]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12496..12497]\n_sock_flush 32 queued reqs\n_sock_flush 15 queued reqs\n_sock_check_zcopy 47 pending reqs range [12498..12499]\n_sock_flush 32 queued reqs\n_sock_check_zcopy 32 pending reqs range [12500..12500]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12501..12502]\n_sock_flush 9 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 55 pending reqs range [12503..12504]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12505..12506]\n_sock_flush 32 queued reqs\n_sock_flush 32 queued reqs\n_sock_check_zcopy 64 pending reqs range [12507..12507]\n_sock_flush 7 queued reqs\n_sock_flush 23 queued reqs\n_sock_check_zcopy 62 pending reqs range [12508..12509]\n_sock_flush 9 queued reqs\n_sock_check_zcopy 32 pending reqs range [12510..12511]\n_sock_flush 32 queued reqs\n_sock_flush 7 queued reqs\ntcp.c: 358:spdk_nvmf_tcp_req_get: *ERROR*: Cannot allocate tcp_req on tqpair\u003d0x3300f00\n_sock_flush 25 queued reqs\n_sock_check_zcopy 64 pending reqs range [12512..12513]\n_sock_flush 7 queued reqs\n_sock_check_zcopy 32 pending reqs range [12514..12514]\n_sock_flush 25 queued reqs",
      "parentUuid": "c50029f2_5d18b850",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "61e07b0f_cacb8065",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-17T09:24:06Z",
      "side": 1,
      "message": "here\u0027s the patch to get these print outs:\n\n\ndiff --git a/module/sock/posix/posix.c b/module/sock/posix/posix.c\nindex aedd371c3..abb4ac9d5 100644\n--- a/module/sock/posix/posix.c\n+++ b/module/sock/posix/posix.c\n@@ -493,6 +493,7 @@ _sock_check_zcopy(struct spdk_sock *sock)\n         * belonging to the same sendmsg call are sequential, so once\n         * we encounter one match we can stop looping as soon as a\n         * non-match is found. */\n+       printf(\"%s %d pending reqs range [%d..%d]\\n\", __func__, sock-\u003epending_reqs_num, serr-\u003eee_info, serr-\u003eee_data);\n        for (idx \u003d serr-\u003eee_info; idx \u003c\u003d serr-\u003eee_data; idx++) {\n                found \u003d false;\n                TAILQ_FOREACH_SAFE(req, \u0026sock-\u003epending_reqs, internal.link, treq) {\n@@ -536,6 +537,8 @@ _sock_flush(struct spdk_sock *sock)\n        /* Gather an iov */\n        iovcnt \u003d 0;\n        req \u003d TAILQ_FIRST(\u0026sock-\u003equeued_reqs);\n+       if (req)\n+               printf(\"%s %d queued reqs\\n\", __func__, sock-\u003equeued_reqs_num);\n        while (req) {\n                offset \u003d req-\u003einternal.offset;\n \ndiff --git a/include/spdk_internal/sock.h b/include/spdk_internal/sock.h\nindex ef856f433..d78fac6cd 100644\n--- a/include/spdk_internal/sock.h\n+++ b/include/spdk_internal/sock.h\n@@ -61,6 +61,8 @@ struct spdk_sock {\n        TAILQ_HEAD(, spdk_sock_request) pending_reqs;\n        int                             queued_iovcnt;\n \n+       int                             queued_reqs_num;\n+       int                             pending_reqs_num;\n        struct {\n                uint8_t         closed          : 1;\n                uint8_t         reserved        : 7;\n@@ -126,6 +128,7 @@ spdk_sock_request_queue(struct spdk_sock *sock, struct spdk_sock_request *req)\n {\n        TAILQ_INSERT_TAIL(\u0026sock-\u003equeued_reqs, req, internal.link);\n        sock-\u003equeued_iovcnt +\u003d req-\u003eiovcnt;\n+       sock-\u003equeued_reqs_num++;\n }\n \n static inline void\n@@ -134,6 +137,8 @@ spdk_sock_request_pend(struct spdk_sock *sock, struct spdk_sock_request *req)\n        TAILQ_REMOVE(\u0026sock-\u003equeued_reqs, req, internal.link);\n        assert(sock-\u003equeued_iovcnt \u003e\u003d req-\u003eiovcnt);\n        sock-\u003equeued_iovcnt -\u003d req-\u003eiovcnt;\n+       sock-\u003equeued_reqs_num--;\n+       sock-\u003epending_reqs_num++;\n        TAILQ_INSERT_TAIL(\u0026sock-\u003epending_reqs, req, internal.link);\n }\n \n@@ -144,6 +149,7 @@ spdk_sock_request_put(struct spdk_sock *sock, struct spdk_sock_request *req, int\n        int rc \u003d 0;\n \n        TAILQ_REMOVE(\u0026sock-\u003epending_reqs, req, internal.link);\n+       sock-\u003epending_reqs_num--;",
      "parentUuid": "c9cc1e13_0bb1f998",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "eec97da4_fc3a9640",
        "filename": "/COMMIT_MSG",
        "patchSetId": 12
      },
      "lineNbr": 12,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-12-17T15:08:13Z",
      "side": 1,
      "message": "There is a patch out from Ziye that removes this error message. I will confirm, but I believe what happens is that the initiator side sees the completion and sends a new command that the target then receives and processes before the target has processed the zero copy ack and released the previous request. The code just holds the incoming request until a new request becomes available, so everything works fine. Also the tcp transport has other accounting on the target side to ensure the queue depth requirements haven\u0027t been violated (in a way that allows for the above scenario), so if that isn\u0027t asserting then it\u0027s just this ordering thing you\u0027re seeing.\n\nI think maybe flipping the order of processing EPOLLIN vs EPOLLERR messages on a socket would greatly reduce or even eliminate this entirely. I\u0027ll also do that.",
      "parentUuid": "61e07b0f_cacb8065",
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "83fd4068_433ca5b1",
        "filename": "module/sock/posix/posix.c",
        "patchSetId": 12
      },
      "lineNbr": 543,
      "author": {
        "id": 1016578
      },
      "writtenOn": "2019-12-12T20:06:03Z",
      "side": 1,
      "message": "So what about the crash I came across on earlier version [1], is it gone? or maybe what does this assert serve now when req-\u003einternal.offset is actually the system call index (and 0 is valid value)?\n\n[1] https://review.gerrithub.io/c/spdk/spdk/+/471752/9/module/sock/posix/posix.c#577",
      "range": {
        "startLine": 543,
        "startChar": 3,
        "endLine": 543,
        "endChar": 23
      },
      "revId": "bf42d85f94c60c965c64fcbf8dcc663a468a9988",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}