{
  "comments": [
    {
      "key": {
        "uuid": "b43451ac_f880e681",
        "filename": "lib/event/subsystems/nvmf/nvmf_tgt.c",
        "patchSetId": 13
      },
      "lineNbr": 216,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-01-10T09:15:19Z",
      "side": 1,
      "message": "I\u0027m not sure that changing the return value of this function is necessary if you add the function I suggested and always call it in the transport before calling the user callback. I think it was just a mistake to wait to set the state to activating prior to calling that callback, so that\u0027s all you need to fix here.",
      "revId": "611c837474624bc88b53ef06a7e48a12acf39dff",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "dde24ecd_b0e244ed",
        "filename": "lib/event/subsystems/nvmf/nvmf_tgt.c",
        "patchSetId": 13
      },
      "lineNbr": 216,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-01-10T12:13:51Z",
      "side": 1,
      "message": "Hi Ben,\n\nMy purpose is to differentiate the following two cases:\n\n1  Failure happens in the accepted core, the qpair state is Uninitialized.\n2  The failure happening to the schedule core via Round robin.\n\nAnd you can see our current code in spdk_nvmf_qpair_disconnect. We judge qpair is uninitialized, then we just return. However, even in uninitialized state, it may not fail. Since there is a window. So I need to set it to activating state early. \nIf we call set the state before cb_fn, we still can not differentiate such case. And set the state before cb_fn, cannot solve the window I mentioned.",
      "parentUuid": "b43451ac_f880e681",
      "revId": "611c837474624bc88b53ef06a7e48a12acf39dff",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "de9be972_52965600",
        "filename": "lib/event/subsystems/nvmf/nvmf_tgt.c",
        "patchSetId": 13
      },
      "lineNbr": 216,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-01-10T15:19:00Z",
      "side": 1,
      "message": "I\u0027m still not following your explanation for the two scenarios, but having thought about this code path I think I may see a scenario where there is a problem. Tell me if this is the same thing you are describing:\n\n1) A new connection is created in nvmf_tgt_accept and the new_qpair_fn is called. That function returns immediately, without yet adding the qpair to a poll group.\n\n2) The qpair is disconnected by the initiator, generating an RDMA_CM_EVENT_DISCONNECT. The qpair does not yet have a poll group set, so it \"spins\" by sending messages to itself over and over waiting for that to happen.\n\n3) The user application either fails to add the qpair to a poll group or decides to exit, so it calls spdk_nvmf_qpair_disconnect (from any thread, potentially)\n\nAt this point, the loop from step #2 will never exit.\n\nIn order to properly address this, some really tricky threading cases need to be handled. I think we need to modify the code so that if a qpair does not have a poll group assigned, we perform all of the management operations (including disconnect) on the acceptor thread. The problem is that we don\u0027t currently capture the pointer to the acceptor thread today, so that\u0027s going to need to change.\n\nI think we also need something like the refcnt mechanism for calls to the new_qpair_fn to coordinate outstanding messages there with disconnect processing.",
      "parentUuid": "dde24ecd_b0e244ed",
      "revId": "611c837474624bc88b53ef06a7e48a12acf39dff",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "d0beb88c_adaf209f",
        "filename": "lib/event/subsystems/nvmf/nvmf_tgt.c",
        "patchSetId": 13
      },
      "lineNbr": 216,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-01-11T01:38:54Z",
      "side": 1,
      "message": "Hi Ben,\n2) is really a problem, which I also described in the discussions in github issues.The polling group will never be added. However, in our current code, we have pre assign the polling group to qpair.\n\nMy problem is that, for new_qpair_fn, part of is still accepted in the acceptor_core. I want to divide the functions into two parts, so I use a return value. \n(1)if there is issue for the part of the code in the acceptor core, we just reject this qpair, since we will never call the spdk_event_call;\n(2) If there is issue related with adding into the group, it is another issue.\nSo currently, our handling for (1)(2) are mixed.\nSince for part(1), if there is qpair creating issue, we can just send the message to the clients early, and say the connection is not ok, then we can avoid the unexpected disconnect. \n\nAnd for the part(2), there is another issue we want to address as you described.",
      "parentUuid": "de9be972_52965600",
      "revId": "611c837474624bc88b53ef06a7e48a12acf39dff",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "f424c3e3_2a83dc9e",
        "filename": "lib/nvmf/nvmf_internal.h",
        "patchSetId": 13
      },
      "lineNbr": 330,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-01-10T09:15:19Z",
      "side": 1,
      "message": "Maybe we should add a function here that looks like this:\n\nstatic inline void\nspdk_nvmf_qpair_initialize(struct spdk_nvmf_qpair *qpair, struct spdk_nvmf_transport *transport)\n{\n    assert(qpair !\u003d NULL);\n    qpair-\u003etransport \u003d transport;\n    qpair-\u003estate \u003d SPDK_NVMF_QPAIR_STATE_ACTIVATING;\n}\n\nThen call this inside each of the transports instead. That way you can leave spdk_nvmf_qpair_set_state where it is and with the two asserts still intact.",
      "revId": "611c837474624bc88b53ef06a7e48a12acf39dff",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": false
    }
  ]
}