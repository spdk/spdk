{
  "comments": [
    {
      "key": {
        "uuid": "2db24f7e_8441846a",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 57,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "DEFAULT\n\nAlso, did you experiment with different sizes here?",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "de192aaa_f56e2302",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 57,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-13T23:12:55Z",
      "side": 1,
      "message": "Yes, I used different sizes.",
      "parentUuid": "2db24f7e_8441846a",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5c0ac14a_b73d3de0",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 1937,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "Let\u0027s make it ok to call this when remain_size is 0 - just return 0 in that case. This will be useful later.",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "35265b15_7ea4e9d7",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 1943,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "Reset off to 0 in nvme_tcp_recv_buf_read when new data arrives (i.e. on line 1924)",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2b71231a_768e87c7",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 1978,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "At the top, before entering this loop, I think we should call nvme_tcp_recv_buf_read just once. I think you should also eliminate current_pdu_num, and instead the exit condition on this function is simply when all of the data read in the initial socket call has been consumed.\n\nThat would mean that pdu_recv_buf.remain_size is always 0 when this function exits. You should add asserts to check that.",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "79163543_7bbe5529",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 1978,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-13T23:12:55Z",
      "side": 1,
      "message": "The loop to count the pdu_num, is designed to balance the execution among the multiple connections in a single group. If you remove it, it will degrade the performance.",
      "parentUuid": "2b71231a_768e87c7",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "64c4f104_b1c57851",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2052,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "I would delete this part where it reads directly from the socket. If we use the strategy where it just does a single read from the socket each time it goes to poll, then this logic can simply resume the next time it polls.",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "844f4eb0_ac4de866",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2052,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-13T23:12:55Z",
      "side": 1,
      "message": "Delete it, will cause performance drop, I already tried it. Since for the incapsule data supported can be 4K or 8K. Removing this, will cause data copy with lots of operations. So for this part, we just read into the allocated data buffer, it will be much better.",
      "parentUuid": "64c4f104_b1c57851",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b720111b_7c34a72a",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-13T21:38:57Z",
      "side": 1,
      "message": "If you use the strategy indicated above where the full recv buffer is processed each time poll is called, you can remove this.",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9224a3dc_89db2f51",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-13T23:12:55Z",
      "side": 1,
      "message": "I did not agree with you since I already tried.Full recv_buffer cannot be achieved. \nReason: \nFor some NVMe commands, we will wait for the data buffer which is allocated from the data buffer. And you need to wait for the buffer to come, then you can handle the data copy from the receive buffer to the shared data buffer. \n\nSo eliminating this code, will cause hang, which I already found. Since next time, there will no data to be polled out by epoll_wait since data is already in the copied shared buffer.",
      "parentUuid": "b720111b_7c34a72a",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "69f3f794_6e22de71",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-15T22:23:13Z",
      "side": 1,
      "message": "If you don\u0027t use the strategy I suggested (and testing is showing that you probably shouldn\u0027t) then I agree you need this.\n\nHowever, I\u0027m slightly concerned about a case that isn\u0027t being tested here. This only \"kicks\" the poller if the whole call to epoll found nothing. What if a user sent a burst of commands on one socket, some of which were queued up for a data buffer, and then consistently kept other sockets busy? Wouldn\u0027t that leave the first socket stuck? I think we need to find a way to trigger calling the poller specifically when a request gets a data buffer after having been queued for one.",
      "parentUuid": "9224a3dc_89db2f51",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "1c750606_43aae7ce",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-16T03:13:38Z",
      "side": 1,
      "message": "Hi Ben,\n\nAs I mentioned. We need this else rc \u003d\u003d 0 case. Another reason is that: \n\nFor multiple connections, according to my test. Limited the handling of PDUs numbers, which can be used to balance performace among the different connections. And it shows the performance is better than exhausting all the recevied buffers.",
      "parentUuid": "69f3f794_6e22de71",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "afb1057f_9a870b7e",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-19T16:53:36Z",
      "side": 1,
      "message": "Hi Ziye - can you read my comment again? I\u0027m no longer disputing that we need this code. I\u0027m pointing out that there may be a bug in the way this is currently working.",
      "parentUuid": "1c750606_43aae7ce",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "6e2281c2_d789c393",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-19T23:22:42Z",
      "side": 1,
      "message": "Hi Ben,\n\nWhat kinds of bug? I removed this code in another patch, and see some performance unbalanced.",
      "parentUuid": "afb1057f_9a870b7e",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8da2b2e5_73e8a3fd",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-20T18:04:22Z",
      "side": 1,
      "message": "From a few comments up:\n\n\"This only \"kicks\" the poller if the whole call to epoll found nothing. What if a user sent a burst of commands on one socket, some of which were queued up for a data buffer, and then consistently kept other sockets busy? Wouldn\u0027t that leave the first socket stuck? I think we need to find a way to trigger calling the poller specifically when a request gets a data buffer after having been queued for one.\"",
      "parentUuid": "6e2281c2_d789c393",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "79550fe3_7fab5221",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-20T23:23:13Z",
      "side": 1,
      "message": "No, it will not. The first one socket will firstly obtain the buffer. Even if the other sockets are busy, they will finally wait for the buffer and wait for the first one socket to complete.\nYou can see the code to wait for the data buffer is in *_group_poll, so the issue you mentioned will never happen.",
      "parentUuid": "8da2b2e5_73e8a3fd",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "1d9585c0_e2867a7e",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1010531
      },
      "writtenOn": "2019-08-21T20:55:00Z",
      "side": 1,
      "message": "I\u0027ll explain it a different way. Say we have two sockets, A and B. On the first call to epoll_wait, both A and B receive a full 8k into their recv buffer. Let\u0027s say they are all read commands for both sockets. First A processes 32 PDUs, but then breaks out, leaving some PDUs unprocessed in the recv buffer. Then B does the same. Then, let\u0027s say the user only ever sends commands on socket B from that point forward. Now, epoll_wait will not return 0 because socket B is busy, but nothing ever comes back to process the remaining PDUs in socket A\u0027s recv_buf. They\u0027re stuck until all sockets go idle, which in the worst case is an infinitely long time.",
      "parentUuid": "79550fe3_7fab5221",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "500f0db5_3d52774e",
        "filename": "lib/nvmf/tcp.c",
        "patchSetId": 6
      },
      "lineNbr": 2829,
      "author": {
        "id": 1011275
      },
      "writtenOn": "2019-08-22T02:44:37Z",
      "side": 1,
      "message": "Hi Ben,\nIf the connection A does not send out the remaining data, the connection A will be disconnected by timeout (either by Admin\u0027s keepalive timeout, or by the TCP socket timeout.). So this is the issue which we already discussed before, not in this scope,right?. For the data already existing in the buffer, my approach will still handle this by the else case. So it will not have the data received but unsolved case.\n\nBy the way, My new code already removes this, could you review on the new code?",
      "parentUuid": "1d9585c0_e2867a7e",
      "revId": "5aec20fd1ed5995badc45430aa7e82a48d6e3c53",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735",
      "unresolved": true
    }
  ]
}